<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PromptKD中文解读</title>
  <link rel="icon" type="image/x-icon" href="None">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</h1>
          <div class="is-size-3 content">
            中文解读
          </div>
          <p class="is-size-5">李政 南开大学/蚂蚁集团</p>
          <br>
          <p class="is-size-5"><a href="https://zhuanlan.zhihu.com/p/684269963">知乎链接</a></p>
        </div>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop ">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">一句话概括</h2>
        <div class="content has-text-justified">
          <p>
            PromptKD是一个简单有效的基于prompt的视觉语言模型蒸馏新方法，在prompt learning的11个benchmark数据集上大幅领先，达到了SOTA。
          </p>
          <h2 class="title is-4">大白话背景介绍</h2>
        
          <p>已经很了解VLMs和prompt learning的同学可以直接跳过，到<strong>背景问题</strong>～</p>
          <p>这里的介绍目的是让没有相关基础和背景的同学也可以看懂这篇工作，能有所收获。</p>

          <p><strong>什么是视觉-语言模型(Vision-Language Models, VLMs)？</strong></p>
          <p>视觉语言模型VLM一般由两个部分构成，即视觉(Vision)部分和语言(Language)部分。以一个经典的VLM网络 CLIP[1] 的结构为例：</p>
          
          <figure>
            <img src="static/interpretation_images/fig1_clip.png" alt="图挂了= =" width="70%">
            <figcaption>图1.CLIP架构。图片来自于CLIP论文。</figcaption>
          </figure>

          <p>如图1所示，CLIP由text branch和image branch组成。</p>

          <p>其中，text branch主要由transformer构成，当要进行cls_num个类的分类任务时，会取每个类别对应的名称，如"plane", "car", "dog"，与"a photo of a"进行组合，作为prompt输入进text encoder，得到大小为[cls_num, feat_dim]的text feature。

          <p>image branch的核心就是对输入的图像提取image feature，其通常为ResNet或者ViT[2]。图像经过image encoder之后得到image feature，其大小为[batch_size, feat_dim]。

          <p>将两个feature进行相乘就得到了预测logits。

          <p>CLIP有两个明确的特性，是这个工作的基础：

          <p>1. CLIP可以进行zero-shot分类，即对未见过的类别进行识别，并保持很高的性能。而传统的CNN或者ViT由于模型架构限制不可以。
          <p> 2.对于已知的类别，CLIP的text branch只需要一次forward就可以得到对应text feature用于分类。

          <strong><p>什么是提示学习(Prompt Learning)？</p></strong>

          <p>在Text Branch部分中，a photo of a {class_name} 这样的描述太过宽泛，明显不是最优的。例如对于图2(b)的花，手工设计的a flower photo a {class}要描述的更加精确，其产生的结果就更好。</p>

          <figure>
            <img src="static/interpretation_images/fig2_prompt.png" alt="图挂了= =" width="100%">
            <figcaption>图2.蓝色方块代表手动设计的prompt，绿色方块代表网络学习得到的learnable prompt。绿色方块acc超越了蓝色。图片来自于CoOp论文。</figcaption>
          </figure>

          <p>这就产生来两个问题，第一，固定模板的prompt不是最优的。第二，针对性的手工设计费时费力，且无法泛化。

          <p>于是，提示学习(Prompt Learning)[3] [4]就提出将prompt变成了一种learnable的方式，通过优化的方法让prompt在下游数据集上学习适用的表征，来替代手工设计的prompt，参考图2中的绿色方块。

          <p>这样优势是，可以在少量数据的情况下，仅通过引入一少部分的可学习参数（即learnable prompt），就可以将原始的CLIP快速适用到下游的任务/数据，同时在性能上比全参数微调的结果更好[4]。

          <strong><p>实验衡量指标是什么？</p></strong>

          <p>有三个指标，分别是base acc，novel acc和harmonic mean。</p>

          <p>以imagenet-1k数据集为例，会取1000类中的前500类作为base class，后500类作为novel class。模型在base class上训练，完成后在base class和novel class上测试acc性能。因为novel class与base class数据类别不重复，所以novel acc可以有效反应模型泛化性能。harmonic mean指标是对base acc和novel acc的综合反映，为harmonic mean = (2*base acc*novel acc) / (base acc+novel acc)。总体的harmonic mean值越高，模型综合性能越好。

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p>
        <h2 class="title is-3">背景问题</h2>
        <p>prompt learning的核心作用是，保持原始CLIP参数不变，通过引入小部分learnable prompt参数，来将大的原始的经过预训练的CLIP模型适用到下游任务/数据上，提升CLIP模型在下游任务的性能，同时保持CLIP模型zero-shot能力。

        <p>除去一直发展至今的各种设计prompt形式的工作[3] [5] [6] [7] [8] [9] [10] [11] [12] [13]，现如今最前沿的prompt learning方法主要还可以分为另外<strong>两类</strong>：
          
        <strong><p>1. 引入额外数据/信息。这一类工作核心就是通过引入额外的数据或信息，做法包括但不限于，</p></strong>
          
        <p>(1).通过LLM来生成{class_name}相关的语句，获得额外的有关{class_name}的特性 特征[14] [15] [16]，或者更多描述性语句[17] [18] [19] [20]。</p>
          
        <p>(2).引入额外的数据源，从wikipedia上引入文本描述[21]，从额外数据集例如ImageNet-21K来做预训练 [22]。
          
        <p>(3).设计给原始图像数据引入额外的tag或标注[23] [24] [25]。
          
        <p>从以上的方式我们看到，大部分引入额外数据信息的工作都是围绕text branch展开，本质原因是输入的text本身"{class_name}"或"a photo of a {classname}"包含信息太少，丰富度要远低于image，通过额外的域内文本信息的引入，可以显著增强text feature的质量。
        <strong>所以text feature的质量是关键。</strong>
          
        <p>同时，可以看到，围绕image branch的工作是相对较少的。这时候问题就来了：那我们可不可以用同样的思路来增强image feature呢？
          
        <p>诶，这个方法好！因为互联网内往往存在非常大量的图像数据，很容易获取。
          
        <strong><p>但问题是这些图像往往是没有标注的，没办法用gt训，如果要去进行标注，需要消耗很多的时间或者钱。明显限制了这种方式的应用。</p></strong>
          
        <strong><p>2. 利用原始CLIP自身信息约束模型学习[19] [26] [27] [28] [29] [30] [31]，防止过拟合。</p></strong>
        <br>
        <p>在Prompt learning中，learnable prompt的参数量是相对较少的，在经过大量base class数据训练之后，模型会对base class数据存在过拟合，丧失对novel class的泛化性能。要解决这个问题，一种非常有效的做法就是利用vanilla CLIP来约束带有prompt的模型的学习。</p>
          
        <p>以ICCV 23 PromptSRC为例，如图3所示，</p>

        <figure>
          <img src="static/interpretation_images/fig3_promptsrc.png" alt="图挂了= =" width="90%">
          <figcaption>图3.PromptSRC结构图。图片来自于PromptSRC论文。</figcaption>
        </figure>

        <p>图3这篇工作就看两条线，蓝线和灰线。</p>

        <p>蓝线，就是原始CLIP的前向计算路径，分别会得到对应的image和text feature。</p>

        <p>灰线，就是带有learnable prompt的计算过程，也会得到对应的feature。</p>

        <p>在两条线的末尾，计算了三个loss，这里就是用原始CLIP产生的image和text feature来约束由含有learnable prompt产生的image和text feature。通过这样的约束，限制了prompt向着base class过拟合，达到了SOTA的性能。</p> 

        <strong><p>由这个工作我们就想，如果换一个更好的模型来做约束是不是性能会更好？</p></strong>
        
        </p>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p>
        <h2 class="title is-3">方法</h2>
        <p>于是，这就引出了我们的工作。</p>

        <p><strong>PromptKD其实核心就在做一件事，引入更大的CLIP模型作为teacher，解决了上面提到的三个问题。</strong></p>
        
        <p>(1) 重用(Reuse) teacher CLIP产生的text feature用于学生的训练和推断。这样确保了text feature高质量的同时，还显著的节省计算量，训练时只涉及student的image encoder。
        
        <p>(2) 对齐学生CLIP和教师CLIP的logits。让大的CLIP模型给小的学生CLIP模型提供更好的监督。
        
        <p>(3) 因为有了教师CLIP的存在，就解决了数据量限制的问题，我们可以用大量的无标签domain data来训学生，不再拘泥于原来有限的有标签数据。在训练时，我们直接可以使用数据集的全量数据作为无标签数据进行蒸馏，这样一来就prompt就可以学到更广泛的domain knowledge。同时高性能的教师CLIP也保证了用于蒸馏的软标签的准确性。
        
        <strong><p>我们先来看一个简单的结构缩略图：</p></strong>

        <figure>
          <img src="static/interpretation_images/fig4_promptkd_short.png" alt="图挂了= =" width="70%">
          <figcaption>图4. PromptKD框架简略图。</figcaption>
        </figure>

        <p>黄色的方块部分代表的就是教师CLIP，在教师CLIP经过训练之后，直接一次forward，得到并保存下来对应类别的text feaure，也就得到了图4中的Pre-stored Text Feature。

        <p>蓝色的方块代表的是学生CLIP，这里其实就只有一个image encoder，在带有learnablr prompt的输入进入image encoder之后会得到对应的image feature，这是因为与teacher text feature在维度上不匹配，所以经过一个Projector，将512转成768维的特征。然后再与Pre-stored Text Feature相乘，得到logits。

        <p>然后进行蒸馏。

        <p>完整的框架图如图5所示：
        
        <figure>
          <img src="static/interpretation_images/fig5_promptkd_overview.png" alt="图挂了= =" width="100%">
          <figcaption>图5. PromptKD整体框架图。</figcaption>
        </figure>

        <p>图5里就是图4过程的细化。

        <p>这里将PromptKD的每个阶段都进行了详细的阐明。大家看图就明白了～

        <p>第一阶段，教师模型的预训练。在这里，我们选择之前的SOTA方法PromptSRC去预训练我们的教师ViT-L/14 CLIP模型，我们的学生模型是ViT-B/16 CLIP模型。

        <p>注意，这里的预训练不是必须的一步，选择去预训练教师模型，是为了让教师有一个更好的性能，从而有更好的学生蒸馏结果。如果直接使用vanilla ViT-L/14 CLIP作为教师，相比于baseline，也取得了明显的性能提升，具体结果请参考表4。

        <p>第二阶段，学生CLIP模型的蒸馏。

        <p>第三阶段，学生的推断。

        <p>最后再来一个简洁明了的流程概括图：

        <figure>
          <img src="static/interpretation_images/fig6_process.png" alt="图挂了= =" width="70%">
          <figcaption>图6. 计算流程。</figcaption>
        </figure>

      </div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
        <p>
        <h2 class="title is-3">实验结果</h2>
        
        <p>我们的PromptKD方法在prompt learning的11个benchmark dataset上都达到了SOTA的性能。

        <p><strong>Base-to-novel实验</strong></p>

        <figure>
          <img src="static/interpretation_images/table1_base2novel.png" alt="图挂了= =" width="100%">
          <figcaption>表1. Base-to-novel实验结果。</figcaption>
        </figure>

        <figure>
          <img src="static/interpretation_images/fig7_hm.png" alt="图挂了= =" width="75%">
          <figcaption>图7. HM分数在11个数据集上的总揽图。</figcaption>
        </figure>

        <p><strong>Cross-dataset实验</strong></p>

        <figure>
          <img src="static/interpretation_images/table2_cross.png" alt="图挂了= =" width="100%">
          <figcaption>表2. Cross-dataset实验结果。</figcaption>
        </figure>

        <p><strong>消融实验</strong></p>

        <p>为了实验快速进行，消融实验里使用的不是全量数据集，而是64 shots per class进行的训练。所以会与表1中的数据相比略低。

        <p>与其他同样使用了无标签数据的工作的性能对比:

        <figure>
          <img src="static/interpretation_images/table3_unsupervise.png" alt="图挂了= =" width="60%">
          <figcaption>表3. 在Flowers102数据集上与使用了无标签数据的其他方法的对比结果。</figcaption>
        </figure>

        <p><strong>教师预训练方法的选择</strong></p>

        <p>在PromptKD中，任意类型的ViT-L/14 CLIP教师模型都可以蒸馏出一个很好的ViT-B/16 CLIP模型，相比于baseline (70.22 HM)都有明显的提升。

        <p>这里有一点非常有意思的是，我们可以看到，第四行的Teacher(CLIP) ViT-L/14也就是原始的CLIP模型，在经过PromptKD的蒸馏之后，我们的ViT-B/16 CLIP的结果（表1(b)）明显超过了原始的ViT-L/14 CLIP模型。(77.62 vs. 76.52)

        <figure>
          <img src="static/interpretation_images/table4_pretrain_method.png" alt="图挂了= =" width="60%">
          <figcaption>表4. 不同教师预训练方法对PromptKD蒸馏效果的影响。</figcaption>
        </figure>

        <p><strong>不同容量教师模型的选择</strong></p>

        <p>如表5所示，绿色代表学生ViT-B/16 CLIP的HM分数，土黄色代表教师的HM分数。教师的性能越高，越能训练出更好的学生。</p>

        <figure>
          <img src="static/interpretation_images/fig8_teachers.png" alt="图挂了= =" width="60%">
          <figcaption>图8. 不同容量的CLIP模型作为教师进行蒸馏。</figcaption>
        </figure>
        <p>欢迎大家试用PrompKD～</p>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths width:100%">
        <div class="content has-text-justified">
        
        <h2 class="title is-3">问题反馈与解答</h2>
        
        <p><strong>1. 问：蒸馏和推理阶段向学生模型的输入中visual prompt在代码中的位置。</strong></p>
        <p> 答：这个在CLIP的代码里已经实现了，在PromptKD/clip/model.py line 366开始以后，line 375的self.VPT就是learnable visual prompt的定义。在forward函数里面line 402就有concat的操作，将visual prompt与image token进行concat输入进ViT进行计算。
        </p>

        <p><strong>2. 问：想要找一个更小backbone的CLIP做蒸馏，只有ResNet-based CLIP了，但是ResNet-based CLIP不支持token形式的learnable prompt，怎么办？</strong></p>
        <p> 
        答：两种方式，第一种，学生模型在这里不是必须要有prompt的，当变成resnet或者更小的模型时，也可以考虑去全参数微调去拟合下游任务。
        第二种，当不支持token形式的prompt时，VPT论文其实给出了方案，就是在spatial的层面去加prompt，另外还可以参考MIT的工作《Exploring Visual Prompts for Adapting Large-Scale Models》，这篇论文的图里给出了很具体的可以应用的visual prompt实现方法，代码 (https://github.com/hjbahng/visual_prompting) 也已经开源了，可以参考去使用。  
        </p>
        
        <p><strong> 3. 问：Teacher CLIP如果没有prompt，不做pretrain可不可以？</strong></p>
        <p> 答：是可以的。其实PromptKD这里的teacher不用局限在到底有没有经过pretrain这个事情上，我们在论文的table 6里也验证了，即使是最原始的ViT-L/14 CLIP用来做蒸馏，也可以取得明显的提升效果。因为promptkd本身是一种纯kd的训练方法，所以teacher的acc其实决定了student学习效果的上限，我们对teacher去进行pre-train，就是在提升这个上限，所以是上限越高蒸馏结果越好。但是如果不做pre-train，也不影响promptkd方法的使用。</p>

        <p><strong> 4. 问：PromptKD和PromptSRC对硬件的需求。</strong></p>
        <p> 答：我的实验是在A100的卡上完成的，所以没有特别在意这个，可能记得不太清楚具体细节了，PromptSRC对于卡的需要还是比较高的，最好是24g的卡，promptkd很省显存，我印象里之前跑某个实验时大概7-8G显存，用11g的1080ti应该就可以跑起来。</p>

        <p><strong> 5. 问：蒸馏阶段的数据如果有真实标签怎么办？</strong></p>
        <p> 答：在本文中，PromptKD受限于论文实验验证标准，使用的是无标签数据进行的蒸馏。而在现实中，如果训练数据包含有gt label，则可以考虑在学生的训练时直接使用gt label，即将图6里算loss这一行只有kd loss的情况换成loss = a* CE(l_stu, gt)+ b * KLD(l_stu, l_tea)进行训练，其中a，b为两项loss的超参，ce为朝着gt优化的cross entropy loss，在训练时可以先固定a=1不动，调整b来进行蒸馏实验，直到发现最优参数。</p>

        <p><strong> 6. 问：蒸馏后的学生模型碰到新的类别怎么办？</strong></p>
        <p> 答：当遇到新类别时，已有的预存储的文本特征已经不再适用，这时可以用教师的text encoder再inference一遍得到新的text feature用来计算。这里需要说明的一点是，这一步重新计算的操作确实存在限制，但这不是promptkd方法层面导致的，而是clip在遇到新的类别的时候都需要进行一次计算。promptkd是在clip本身基础上进行的对已知类别的优化。</p>

        <p><strong> 7. 代码复现问题集合</strong></p>

        <p><strong>(1) 问：结果是单次还是平均？</strong></p>
        <p> 答：基于3个seed得到的结果取平均。</p> 

        <p><strong>(2) 问：尝试自己预训练教师，然后进行蒸馏，结果达不到</strong></p>
        <p> 答：首先需要注意的是，预训练教师是否达到了论文补充材料的table 10里面报告的acc。如果没达到，那么可能蒸馏的效果就不会那么好。（论文的预训练方法是采用promptsrc用默认训练的setting训的VIT-L/14 CLIP）。为了方便复现，github代码里面已经在各种渠道提供了预训练的模型，包括百度云，terabox，google cloud，github的repo下releases部分。推荐使用已有预训练模型进行训练。</p>

        <p><strong>(3) 问：复现结果有波动</strong></p>
        <p> 答：prompt learning领域方法的论文有两个限制：训练数据量小和训练参数量小。这时候会存在一定的训练的波动，推荐多run几个seed，比如seed 1-5，然后去掉训练波动的数据值，取三个结果avg作为最终结果。更推荐大家在数据量大的数据集imagenet上做实验，实验结果会比较稳定。</p>
      </div></div>
    </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered ">
      <div class="column is-four-fifths width:70%">
        <div class="content has-text-justified">
        <p>
        <h2 class="title is-3">致谢</h2>
        <p>这篇论文解读感谢师弟武戈同学的部分论文总结，PromptKD这篇工作感谢我的导师和co-author们，另外还非常感谢蚂蚁的申书恒，张长浩和傅幸同学的讨论和帮助。</p>
      </div></div>
    </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths ">
        <div class="content has-text-justified">
        <p>
        <h2 class="title is-3">参考文献</h2>
        [1] Learning Transferable Visual Models From Natural Language Supervision ICML 21.  
        <br>
        [2] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 21.  
        <br>
        [3] Learning to Prompt for Vision-Language Models. IJCV 22.  
        <br>
        [4] Visual Prompt Tuning. ECCV 22.
        <br>  
        [5] Conditional Prompt Learning for Vision-Language Models. CVPR 22.
        <br>
        [6] Learning to decompose visual features with latent textual prompts. ICLR 23.
        <br>
        [7] Exploring Visual Prompts for Adapting Large-Scale Models. arxiv 22.
        <br>
        [8] Diversity-Aware Meta Visual Prompting. CVPR 23.
        <br>
        [9] MaPLe: Multi-modal Prompt Learning. CVPR 23.
        <br>
        [10] Read-only Prompt Optimization for Vision-Language Few-shot Learning. ICCV 23.
        <br>
        [11] Task Residual for Tuning Vision-Language Models. CVPR 23.
        <br>
        [12] SA2VP: Spatially Aligned-and-Adapted Visual Prompt. AAAI 24.
        <br>
        [13] LaViP: Language-Grounded Visual Prompts. AAAI 24.
        <br>
        [14] Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models. AAAI 24.
        <br>
        [15] Visual Classification via Description from Large Language Models. ICML 23.
        <br>
        [16] ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models. arxiv 23.
        <br>
        [17] What does a platypus look like? Generating customized prompts for zero-shot image classification. ICCV 23.
        <br>
        [18] Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners. CVPR 23.
        <br>
        [19] Learning to Prompt with Text Only Supervision for Vision-Language Models. arxiv 24.
        <br>
        [20] Waffling around for Performance: Visual Classification with Random Words and Broad Concepts. ICCV 23.
        <br>
        [21] Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models. ICCV 23.
        <br>
        [22] Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition. NIPS 23.
        <br>
        [23] What does CLIP know about a red circle? Visual prompt engineering for VLMs. ICCV 23.
        <br>
        [24] Fine-grained visual prompting. NIPS 23.
        <br>
        [25] LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models. ICCV 23.
        <br>
        [26] Prompt-aligned Gradient for Prompt Tuning. ICCV 23.
        <br>
        [27] Visual-Language Prompt Tuning with Knowledge-guided Context Optimization. CVPR 23.
        <br>
        [28] Consistency-guided prompt learning for vision-language models. ICLR 24.
        <br>
        [29] Self-regulating Prompts: Foundational Model Adaptation without Forgetting. ICCV 23.
        <br>
        [30] LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models. CVPR 23.
        <br>
        [31] Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization. NIPS 23.
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths width:100%">

      <div class="column">
        <div class="content has-text-centered">
        <h2 class="title">以上为全部内容解读</h2>
      </pre>
      </div>
      </div>
    </div>
    </div>
  </div>
</section>


</body>
</html>
